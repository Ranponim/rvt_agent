"""
LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì„¤ì • ëª¨ë“ˆ

OpenAI í˜¸í™˜ LLM (ì˜ˆ: LM Studio, Ollama ë“±) í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
ì„¤ì •ê°’ì€ app.core.config.settingsì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""

import logging
from langchain_openai import ChatOpenAI
from app.core.config import settings

logger = logging.getLogger(__name__)

def get_llm():
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë°˜í™˜
    
    Returns:
        ChatOpenAI: ì„¤ì •ëœ LLM ì¸ìŠ¤í„´ìŠ¤
    """
    try:
        logger.info(f"ğŸ¤– LLM ì´ˆê¸°í™” ì‹œì‘ (Model: {settings.AGENT_MODEL_NAME}, URL: {settings.AGENT_API_URL})")
        
        llm_instance = ChatOpenAI(
            model=settings.AGENT_MODEL_NAME,
            base_url=settings.AGENT_API_URL,
            api_key="not-needed"
        )
        
        logger.debug("âœ… LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
        return llm_instance
        
    except Exception as e:
        logger.error(f"âŒ LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise

# ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤ (lazy initializationì„ ìœ„í•´ í•„ìš”ì‹œ get_llm() í˜¸ì¶œ ê¶Œì¥)
llm = get_llm()

