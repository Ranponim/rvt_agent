import os
import glob
import logging
from typing import List, Dict, Any, Optional

import chromadb
from chromadb.config import Settings as ChromaSettings
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer

from app.core.config import settings

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class RAGService:
    """
    RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤
    
    - Vector DB (ChromaDB) ê´€ë¦¬ ë° ê²€ìƒ‰ ë‹´ë‹¹
    - ë¡œì»¬ ì„ë² ë”© ëª¨ë¸(SentenceTransformer)ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì„ë² ë”© ìˆ˜í–‰
    - ê¸°ìˆ  ë¬¸ì„œë¥¼ ì²­í¬(Chunk) ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì¸ë±ì‹±
    - Singleton íŒ¨í„´ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ë³´ì¥
    """
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RAGService, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
            
        self.embedding_model_name = "jhgan/ko-sroberta-multitask"
        self.chroma_dir = settings.CHROMA_DB_DIR
        self.collection_name = "peg_knowledge_base"
        
        # 1. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (Lazy Loading)
        # ì´ˆê¸° ê¸°ë™ ì†ë„ë¥¼ ìœ„í•´ ëª¨ë¸ ë¡œë”©ì€ ì‹¤ì œ í•„ìš” ì‹œì ê¹Œì§€ ì§€ì—°
        self.model = None 

        # 2. ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if not os.path.exists(self.chroma_dir):
            os.makedirs(self.chroma_dir)
            logger.info(f"ğŸ“‚ ChromaDB ë””ë ‰í† ë¦¬ ìƒì„±: {self.chroma_dir}")
            
        self.client = chromadb.PersistentClient(path=self.chroma_dir)
        
        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„± (Cosine Similarity ì‚¬ìš©)
        self.collection = self.client.get_or_create_collection(
            name=self.collection_name,
            metadata={"hnsw:space": "cosine"} 
        )
        
        self.kb_dir = os.path.join(settings.KNOWLEDGE_BASE_DIR, "peg_docs")
        self._initialized = True
        logger.info(f"âœ… RAGService ì´ˆê¸°í™” ì™„ë£Œ (Collection: {self.collection_name})")

    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (Lazy Loading)"""
        if self.model is None:
            logger.info(f"â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.embedding_model_name}...")
            self.model = SentenceTransformer(self.embedding_model_name)
            logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def initialize_knowledge_base(self, force_reload: bool = False):
        """
        ì§€ì‹ ë² ì´ìŠ¤ ì´ˆê¸°í™” (Initialize Knowledge Base)
        
        ê¸°ìˆ  ë¬¸ì„œ(MD íŒŒì¼)ë“¤ì„ ë¡œë“œí•˜ê³  ì²­í‚¹(Chunking)í•˜ì—¬ Vector DBì— ì¸ë±ì‹±í•©ë‹ˆë‹¤.
        
        Args:
            force_reload (bool): ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ê°•ì œë¡œ ì¬êµ¬ì¶•í• ì§€ ì—¬ë¶€
        """
        try:
            # DBê°€ ë¹„ì–´ìˆì§€ ì•Šê³  ê°•ì œ ë¦¬ë¡œë“œê°€ ì•„ë‹ˆë©´ ê±´ë„ˆëœ€
            if self.collection.count() > 0 and not force_reload:
                logger.info(f"â„¹ï¸ Vector DBì— ì´ë¯¸ {self.collection.count()}ê°œì˜ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return

            self._load_model()
            
            logger.info("ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì‘ (from MD files)...")
            
            # 1. ë¬¸ì„œ ë¡œë“œ
            md_files = glob.glob(os.path.join(self.kb_dir, "*.md"))
            
            if not md_files:
                logger.warning(f"âš ï¸ ì§€ì‹ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ì— MD íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.kb_dir}")
                return

            # í—¤ë” ê¸°ë°˜ ë¶„í•  ì„¤ì •
            headers_to_split_on = [
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ]
            markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
            # ë¬¸ì ìˆ˜ ê¸°ë°˜ ì¶”ê°€ ë¶„í•  ì„¤ì •
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

            ids = []
            documents = []
            metadatas = []

            for file_path in md_files:
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        text = f.read()
                    
                    # 1ì°¨: í—¤ë”ë³„ ë¶„í• 
                    header_splits = markdown_splitter.split_text(text)
                    
                    # 2ì°¨: ë¬¸ì ìˆ˜ë³„ ì¶”ê°€ ë¶„í• 
                    final_splits = text_splitter.split_documents(header_splits)
                    
                    for idx, split in enumerate(final_splits):
                        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        source = os.path.basename(file_path)
                        meta = split.metadata.copy()
                        meta["source"] = source
                        
                        # ë¬¸ì„œ ë‚´ìš©
                        content = split.page_content
                        
                        # ê³ ìœ  ID ìƒì„±
                        doc_id = f"{source}_{idx}"
                        
                        ids.append(doc_id)
                        documents.append(content)
                        metadatas.append(meta)
                        
                except Exception as e:
                    logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({file_path}): {e}")
                    continue

            if not documents:
                logger.warning("âš ï¸ ì¶”ì¶œëœ ë¬¸ì„œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            # 2. ë¬¸ì„œ ì„ë² ë”©
            logger.info(f"â³ {len(documents)}ê°œì˜ ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.model.encode(documents).tolist()
            
            # 3. ChromaDB ì €ì¥
            # ê°•ì œ ë¦¬ë¡œë“œ ì‹œ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
            if force_reload and self.collection.count() > 0:
                 logger.info("ğŸ”„ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘...")
                 self.client.delete_collection(self.collection_name)
                 self.collection = self.client.create_collection(
                    name=self.collection_name, 
                    metadata={"hnsw:space": "cosine"}
                )

            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ê°€ (Batch Processing)
            batch_size = 100
            for i in range(0, len(ids), batch_size):
                end_idx = min(i + batch_size, len(ids))
                self.collection.add(
                    ids=ids[i:end_idx],
                    documents=documents[i:end_idx],
                    embeddings=embeddings[i:end_idx],
                    metadatas=metadatas[i:end_idx]
                )
                
            logger.info(f"âœ… ì§€ì‹ ë² ì´ìŠ¤ ì¸ë±ì‹± ì™„ë£Œ: ì´ {len(ids)}ê°œ ì²­í¬ ì €ì¥ë¨.")
            
        except Exception as e:
            logger.error(f"âŒ ì§€ì‹ ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)

    def search(self, query: str, k: int = 3) -> List[str]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ (Search)
        
        ì‚¬ìš©ì ì§ˆì˜(Query)ì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì„œë¥¼ Vector DBì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            query (str): ê²€ìƒ‰ ì§ˆì˜
            k (int): ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            List[str]: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ë¦¬ìŠ¤íŠ¸
        """
        try:
            self._load_model()
            
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.model.encode([query]).tolist()
            
            # ë²¡í„° ê²€ìƒ‰
            results = self.collection.query(
                query_embeddings=query_embedding,
                n_results=k
            )
            
            # results['documents']ëŠ” List[List[str]] í˜•íƒœì„
            if results and results['documents']:
                documents = results['documents'][0]
                # logger.debug(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ({k}ê±´): {documents}") # ë‚´ìš©ì´ ë§ì•„ ë¡œê·¸ ë ˆë²¨ ì£¼ì˜
                return documents
            
            return []
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            return []

# Singleton Instance
rag_service = RAGService()
